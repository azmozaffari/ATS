{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "275f6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import random \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1609fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reproduce  the experiment's results , set the seed \n",
    "SEED = 42\n",
    "\n",
    "def deterministic(seed):\n",
    "    \"\"\"\n",
    "    Setup execution state so that we can reproduce multiple executions.\n",
    "    Make the execution \"as deterministic\" as possible.\n",
    "\n",
    "    random_seed: seed used to feed torch, numpy and python random\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)      \n",
    "       \n",
    "    \n",
    "deterministic(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1ad2860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# If there is any GPU use that for computing\n",
    "def set_device_cuda():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' # the hardware device\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "    return device\n",
    "device = set_device_cuda()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e5fbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet model\n",
    "\"\"\"\n",
    "DenseNet for cifar with pytorch\n",
    "\n",
    "Reference:\n",
    "[1] H. Gao, Z. Liu, L. Maaten and K. Weinberger. Densely connected convolutional networks. In CVPR, 2017\n",
    "\"\"\"\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet_Cifar(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "    def __init__(self, growth_rate=12, block_config=(16, 16, 16),\n",
    "                 num_init_features=24, bn_size=4, drop_rate=0, num_classes=10):\n",
    "\n",
    "        super(DenseNet_Cifar, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # initialize conv and bn parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=8, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet_BC_cifar(depth, k, **kwargs):\n",
    "    N = (depth - 4) // 6\n",
    "    model = DenseNet_Cifar(growth_rate=k, block_config=[N, N, N], num_init_features=2*k, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1253d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Dataloader for CIFAR100\n",
    "def dataLoader( is_train=True,  batch_size=64, shuffle=True):\n",
    "    \n",
    "        loader = dict()\n",
    "        if is_train:\n",
    "            trans = [transforms.RandomHorizontalFlip(),\n",
    "                     transforms.RandomCrop(32, padding=4),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize(mean=[n/255.\n",
    "                        for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])]\n",
    "            trans = transforms.Compose(trans)\n",
    "            train_set = datasets.CIFAR100('data',download=True, train=True, transform=trans)\n",
    "#             print(len(train_set))\n",
    "            train_dataset, val_dataset, test_dataset = random_split(train_set, (len(train_set)-1000, 1000,0))\n",
    "            print(len(train_dataset), len(val_dataset))\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                            train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                            val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "            loader = {'train_loader':train_loader,'val_loader':val_loader}\n",
    "        else:\n",
    "            trans = [transforms.ToTensor(),\n",
    "                     transforms.Normalize(mean=[n/255.\n",
    "                        for n in [129.3, 124.1, 112.4]], std=[n/255. for n in [68.2,  65.4,  70.4]])]\n",
    "            trans = transforms.Compose(trans)\n",
    "            test_set = datasets.CIFAR100('data',download=True, train=False, transform=trans)\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                            test_set, batch_size=batch_size, shuffle=shuffle)\n",
    "            \n",
    "            loader = {'test_loader':train_loader}\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b796d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/cifar-100-python.tar.gz\n",
      "Extracting data/cifar-100-python.tar.gz to data\n",
      "49000 1000\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define the CIFAR100 dataset and dataloader\n",
    "train = dataLoader( is_train=True,  batch_size=64, shuffle=True)\n",
    "test = dataLoader( is_train=False,  batch_size=64, shuffle=True)\n",
    "\n",
    "train_loader =train['train_loader']\n",
    "val_loader = train['val_loader']\n",
    "test_loader = test['test_loader']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bf4785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_samples(pred,label):\n",
    "    y_hat = torch.argmax(pred,dim=1)\n",
    "#     print(y_hat,label)\n",
    "    correct_answers = (torch.where(y_hat==label))[0].shape[0]\n",
    "    return correct_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1669736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights in the model with uniform distribution\n",
    "\n",
    "# train the model and save it \n",
    "model = DenseNet_Cifar(block_config=(6, 6, 6), num_classes=100)\n",
    "model = model.to(device)\n",
    "\n",
    "# torch.nn.init.uniform_(model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epoch_numbers = 200\n",
    "\n",
    "\n",
    "for epoch in range(epoch_numbers):\n",
    "    csamples_train = 0\n",
    "    total_loss_train = 0\n",
    "    csamples_val = 0\n",
    "    total_loss_val = 0\n",
    "    \n",
    "    model.train()\n",
    "    train_samples = 0\n",
    "    val_samples = 0\n",
    "    \n",
    "    \n",
    "    for img,label in train_loader:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        train_samples = img.shape[0]+train_samples\n",
    "        pred = model(img)\n",
    "        loss = loss_function(pred,label)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        csamples_train = csamples_train + correct_samples(pred,label)\n",
    "        total_loss_train = total_loss_train +loss.item()\n",
    "    \n",
    "    \n",
    "    print(\"epoch: \", epoch, \" train_loss = \", total_loss_train/(len(train_loader)), \n",
    "          'train accuracy = ', csamples_train/(train_samples)\n",
    "         )\n",
    "\n",
    "    model.eval()\n",
    "    for img,label in val_loader:\n",
    "        \n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = model(img)\n",
    "        loss = loss_function(pred,label)\n",
    "        val_samples = val_samples +img.shape[0]\n",
    "        \n",
    "        \n",
    "        csamples_val = csamples_val + correct_samples(pred,label)\n",
    "        total_loss_val = total_loss_val +loss.item()\n",
    "    \n",
    "    \n",
    "    print(\"epoch: \", epoch, \" val_loss = \", total_loss_val/(len(val_loader)), \n",
    "          'val accuracy = ', csamples_val/(val_samples)\n",
    "         )\n",
    "    \n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        PATH = './model/model_'+str(epoch)+'.pth'\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f07751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9181b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
