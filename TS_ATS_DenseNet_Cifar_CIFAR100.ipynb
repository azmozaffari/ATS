{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.autograd as autograd\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def deterministic(seed):\n",
    "    \"\"\"\n",
    "    Setup execution state so that we can reproduce multiple executions.\n",
    "    Make the execution \"as deterministic\" as possible.\n",
    "\n",
    "    random_seed: seed used to feed torch, numpy and python random\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)      \n",
    "       \n",
    "    \n",
    "deterministic(SEED)\n",
    "    \n",
    "    \n",
    "    \n",
    "class _ECELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Expected Calibration Error of a model.\n",
    "    (This isn't necessary for temperature scaling, just a cool metric).\n",
    "    The input to this loss is the logits of a model, NOT the softmax scores.\n",
    "    This divides the confidence outputs into equally-sized interval bins.\n",
    "    In each bin, we compute the confidence gap:\n",
    "    bin_gap = | avg_confidence_in_bin - accuracy_in_bin |\n",
    "    We then return a weighted average of the gaps, based on the number\n",
    "    of samples in each bin\n",
    "    See: Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht.\n",
    "    \"Obtaining Well Calibrated Probabilities Using Bayesian Binning.\" AAAI.\n",
    "    2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece\n",
    "       \n",
    "def get_nll_ece(logits, logits_scaled, labels):\n",
    "    nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "    ece_criterion = _ECELoss(n_bins=10).cuda()\n",
    "    \n",
    "    nll0 = nll_criterion(logits, labels).item()\n",
    "    ece0 = ece_criterion(logits, labels).item()\n",
    "\n",
    "    nll1 = nll_criterion(logits_scaled, labels).item()\n",
    "    ece1 = ece_criterion(logits_scaled, labels).item()\n",
    "\n",
    "    print(f'+-----+---------+---------+')\n",
    "    print(f'|     | Before  |  After  |')\n",
    "    print(f'+-----+---------+---------+')\n",
    "    print(f'| NLL | {nll0:7.5f} | {nll1:6.5f} |')\n",
    "    print(f'| ECE | {ece0:7.5f} | {ece1:6.5f} |')\n",
    "    print(f'+-----+---------+---------+')\n",
    "\n",
    "    return (nll0, ece0), (nll1, ece1)    \n",
    "    \n",
    "def get_logits_labels(model, data_loader):\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, label in data_loader:\n",
    "            input = input.cuda()\n",
    "            logits = model(input)\n",
    "            logits_list.append(logits)\n",
    "            labels_list.append(label.long())\n",
    "        logits = torch.cat(logits_list).cuda()\n",
    "        labels = torch.cat(labels_list)\n",
    "\n",
    "    labels = torch.cuda.LongTensor([label for idx, label in np.argwhere(labels.numpy())])\n",
    "    \n",
    "    return logits, labels  \n",
    "\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    A thin decorator, which wraps a model with temperature scaling\n",
    "    model (nn.Module):\n",
    "        A classification neural network\n",
    "        NB: Output of the neural network should be the classification logits,\n",
    "            NOT the softmax (or log softmax)!\n",
    "    \"\"\"\n",
    "    def __init__(self, model, initial_temperature=1.5):\n",
    "        super(ModelWithTemperature, self).__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * initial_temperature)\n",
    "        self.cuda()\n",
    "        self.nll_criterion = nn.CrossEntropyLoss().cuda()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Perform temperature scaling on logits\n",
    "        \"\"\"\n",
    "        # Expand temperature to match the size of logits\n",
    "        temperature = self.temperature.unsqueeze(1).expand(logits.size(0), logits.size(1))\n",
    "        return logits / temperature            \n",
    "    \n",
    "    # This function probably should live outside of this class, but whatever\n",
    "    def set_temperature(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Tune the tempearature of the model (using the validation set).\n",
    "        We're going to set it to optimize NLL.\n",
    "        data_loader (DataLoader): validation set loader\n",
    "        \"\"\"                \n",
    "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "        \n",
    "        def eval():\n",
    "            loss = self.nll_criterion(self.temperature_scale(logits), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(eval)\n",
    "        \n",
    "        print('Optimal temperature = %.3f' % self.temperature.item())\n",
    "        \n",
    "class ATS(nn.Module):\n",
    "\n",
    "    def __init__(self, initial_temperature=1.5):\n",
    "        super().__init__()\n",
    "        self.temperature = autograd.Variable(torch.ones(1)*initial_temperature, requires_grad=True)\n",
    "        \n",
    "    def attended_NLL(self,logits, considered_labels, true_labels):\n",
    "        soft_logit = F.softmax(logits , dim=1).cuda()\n",
    "        loss = 0\n",
    "        for i in range(soft_logit.shape[0]):\n",
    "            if (true_labels[i].item() == considered_labels[i].item()):\n",
    "                loss = loss - 0.001*torch.log((((soft_logit[i,considered_labels[i]]).cuda()+(1e-8))/(1-(soft_logit[i,considered_labels[i]]).cuda()+(1e-8)))*(1-soft_logit[i,true_labels[i]].cuda()+(1e-8)))\n",
    "            else:\n",
    "                loss = loss - 0.001*torch.log((((soft_logit[i,considered_labels[i]]).cuda()+(1e-8))/(1-(soft_logit[i,considered_labels[i]]).cuda()+(1e-8)))*(soft_logit[i,considered_labels[i]].cuda()+(1e-8)))\n",
    "        return loss\n",
    "\n",
    "    def select_approperate_samples_for_ATS(self, logits, labels, threshold):\n",
    "        considered_class = []\n",
    "        considered_class_labels = []\n",
    "        true_labels = []\n",
    "        confidence = F.softmax(logits,dim=1)\n",
    "\n",
    "        for i in range(torch.max(labels)+1):\n",
    "\n",
    "            considered_class.append(logits[(labels == i),:])        \n",
    "            considered_class_labels.append(labels[(labels == i)])\n",
    "            true_labels.append(labels[(labels == i)])\n",
    "\n",
    "            b = torch.mul((logits.max(1)[1]==i),(labels!=i))\n",
    "            \n",
    "\n",
    "            considered_class.append(logits[(b==1),:])\n",
    "            lab = torch.ones(labels[(b==1)].size()[0], dtype=torch.long)*i\n",
    "            lab = lab.cuda()\n",
    "            considered_class_labels.append(lab) \n",
    "            true_labels.append(labels[(b==1)]) \n",
    "\n",
    "            b = torch.mul((logits.max(1)[1]!=i),(labels!=i))\n",
    "            c = torch.mul(b,(confidence[:,i]>threshold))\n",
    "            considered_class.append(logits[(c==1),:])\n",
    "            lab = torch.ones(labels[(c==1)].size()[0], dtype=torch.long)*i\n",
    "\n",
    "            lab = lab.cuda()\n",
    "\n",
    "            considered_class_labels.append(lab) \n",
    "            true_labels.append(labels[(c==1)]) \n",
    "\n",
    "        considered_class = torch.cat(considered_class)\n",
    "        considered_class_labels = torch.cat(considered_class_labels)    \n",
    "        true_labels = torch.cat(true_labels)\n",
    "\n",
    "        return considered_class,considered_class_labels, true_labels\n",
    "\n",
    "    def find_best_T(self, logits, labels, threshold):\n",
    "        considered_class,considered_labels, true_labels = self.select_approperate_samples_for_ATS(logits, labels, threshold)\n",
    "        logits = logits.cuda()\n",
    "        considered_class = considered_class.cuda()\n",
    "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval():\n",
    "            L = considered_class / self.temperature.unsqueeze(1).cuda()\n",
    "            loss = self.attended_NLL(L,considered_labels, true_labels).cuda()\t\t\t\n",
    "            loss = loss.cuda()\n",
    "            loss.backward(retain_graph=True )\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(eval)\n",
    "        return self.temperature\n",
    "    \n",
    "def split_eval_dataset(model, dataset, validation_size=0.1, validation_labels_noise=0):\n",
    "    \"\"\"\n",
    "    Evaluates the whole dataset using the given model and splits the logits and labels into two datasets\n",
    "    using the validation_size proportion. If you want to add noise on the validation set labels, use the \n",
    "    parameter validation_labels_noise with range [0,1].\n",
    "    \"\"\"\n",
    "    idxs = range(len(dataset))\n",
    "    idxsvalid, idxstest = train_test_split(idxs, test_size=(1-validation_size), random_state=42, stratify=dataset.labels)\n",
    "    dsvalid = Subset(dataset, idxsvalid)\n",
    "    valid_loader = DataLoader(dsvalid, 128, num_workers=1, pin_memory=True)\n",
    "    dstest = Subset(dataset, idxstest)\n",
    "    test_loader = DataLoader(dstest, 128, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    logits_valid, labels_valid = get_logits_labels(model, valid_loader)\n",
    "    logits_test, labels_test = get_logits_labels(model, test_loader)\n",
    "    \n",
    "    # Apply noise randomly\n",
    "    if validation_labels_noise > 0:\n",
    "        num_classes = logits_valid.shape[-1]-1\n",
    "        idxs_valid = list(range(len(labels_valid)))\n",
    "        random.shuffle(idxs_valid)\n",
    "        for idx in idxs_valid[:int(validation_labels_noise*len(idxs_valid))]:\n",
    "            labels_valid[idx] = random.randint(0,num_classes)\n",
    "        #labels_valid = torch.cuda.LongTensor([random.randint(0,num_classes) if random.random() < validation_labels_noise else label for label in labels_valid])\n",
    "    \n",
    "    return logits_valid, labels_valid, logits_test, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "class CIFAR100(Dataset):\n",
    "    \"\"\"\n",
    "    Download CIFAR100 from https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "    After extraction, reference the dataset root using the parameter base_dir.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"./data\", dataset='train', offset=0, limit=None, verbose=True):\n",
    "        self.set = unpickle(f'{base_dir}/cifar100/{dataset}')\n",
    "        self.meta = unpickle(f'{base_dir}/cifar100/meta')\n",
    "        self.fine_label_names = [t.decode('utf8') for t in self.meta[b'fine_label_names']]\n",
    "        self.data = self.set[b'data']\n",
    "\n",
    "        self.labels = np.zeros((len(self.data), 100))\n",
    "        for idx, label in enumerate(self.set[b'fine_labels']):\n",
    "            self.labels[idx][label] = 1\n",
    "\n",
    "        self.offset = offset if offset > 0 else 0\n",
    "        self.limit = len(self.data) if limit is None else limit\n",
    "\n",
    "        self.data = self.data[offset:limit]\n",
    "        self.labels = self.labels[offset:limit]\n",
    "        print(self.labels.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        image = np.zeros((32, 32, 3), dtype=np.uint8)\n",
    "        image[..., 0] = np.reshape(d[:1024], (32, 32))  # Red channel\n",
    "        image[..., 1] = np.reshape(d[1024:2048], (32, 32))  # Green channel\n",
    "        image[..., 2] = np.reshape(d[2048:], (32, 32))  # Blue channel\n",
    "\n",
    "        return transforms.ToTensor()(image), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DenseNet for cifar with pytorch\n",
    "\n",
    "Reference:\n",
    "[1] H. Gao, Z. Liu, L. Maaten and K. Weinberger. Densely connected convolutional networks. In CVPR, 2017\n",
    "\"\"\"\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet_Cifar(nn.Module):\n",
    "    r\"\"\"Densenet-BC model class, based on\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "    \"\"\"\n",
    "    def __init__(self, growth_rate=12, block_config=(16, 16, 16),\n",
    "                 num_init_features=24, bn_size=4, drop_rate=0, num_classes=10):\n",
    "\n",
    "        super(DenseNet_Cifar, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # initialize conv and bn parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=8, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet_BC_cifar(depth, k, **kwargs):\n",
    "    N = (depth - 4) // 6\n",
    "    model = DenseNet_Cifar(growth_rate=k, block_config=[N, N, N], num_init_features=2*k, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNet_Cifar(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=132, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = CIFAR100(base_dir='./data', dataset='test', verbose=True)\n",
    "\n",
    "model = DenseNet_Cifar(block_config=(6, 6, 6), num_classes=100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_state = torch.load('./model/model_70.pth')\n",
    "# model_state = OrderedDict([(key.replace(\"module.\",\"\"), value) for key, value in model_state['state_dict'].items()])\n",
    "model.load_state_dict(model_state)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing TS with ATS exploring different thresholds with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Noise = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_temperature = 3\n",
    "split_size = 0.02\n",
    "\n",
    "logits_valid, labels_valid, logits_test, labels_test = split_eval_dataset(model, dataset, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal temperature = 2.931\n",
      "# validation_size = 200\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.33487 | 3.69636 |\n",
      "| ECE | 0.36361 | 0.03984 |\n",
      "+-----+---------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5.334865093231201, 0.3636055290699005),\n",
       " (3.696359872817993, 0.03984362259507179))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TS\n",
    "model_temp = ModelWithTemperature(model, initial_temperature=initial_temperature) \n",
    "model_temp.set_temperature(logits_valid, labels_valid)\n",
    "\n",
    "logits_valid_scaled_ts = model_temp.temperature_scale(logits_valid)      \n",
    "# logits_test_scaled_ts = model_temp.temperature_scale(logits_test)     \n",
    "\n",
    "print(f\"# validation_size = {len(logits_valid)}\")\n",
    "get_nll_ece(logits_valid, logits_valid_scaled_ts, labels_valid)    \n",
    "print(f\"# test_size = {len(logits_test)}\")\n",
    "get_nll_ece(logits_test, logits_test_scaled_ts, labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal temperature:  2.3912477493286133\n",
      "# validation_size = 200\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.33487 | 3.73758 |\n",
      "| ECE | 0.36361 | 0.06200 |\n",
      "+-----+---------+---------+\n",
      "# test_size = 9800\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.17957 | 3.63603 |\n",
      "| ECE | 0.37831 | 0.05170 |\n",
      "+-----+---------+---------+\n",
      "CPU times: user 34.6 s, sys: 3.98 ms, total: 34.6 s\n",
      "Wall time: 34.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5.179574966430664, 0.3783113360404968),\n",
       " (3.6360340118408203, 0.05169958621263504))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# ATS\n",
    "model_ATS = ATS(initial_temperature=initial_temperature)\n",
    "T_ATS = model_ATS.find_best_T(logits_valid, labels_valid, threshold=1e-5)\n",
    "print(\"Optimal temperature: \",T_ATS.item())\n",
    "\n",
    "logits_valid_scaled_ats = logits_valid/T_ATS.cuda()\n",
    "logits_test_scaled_ats = logits_test/T_ATS.cuda()\n",
    "\n",
    "print(f\"# validation_size = {len(logits_valid)}\")\n",
    "get_nll_ece(logits_valid, logits_valid_scaled_ats, labels_valid)\n",
    "print(f\"# test_size = {len(logits_test)}\")\n",
    "get_nll_ece(logits_test, logits_test_scaled_ats, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Noise = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_temperature = 3\n",
    "split_size = 0.02\n",
    "\n",
    "logits_valid, labels_valid, logits_test, labels_test = split_eval_dataset(model, dataset, split_size,validation_labels_noise=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal temperature = 4.000\n",
      "# validation_size = 200\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.50194 | 3.79190 |\n",
      "| ECE | 0.36361 | 0.06130 |\n",
      "+-----+---------+---------+\n",
      "# test_size = 9800\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.17957 | 3.67798 |\n",
      "| ECE | 0.37831 | 0.07266 |\n",
      "+-----+---------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5.179574966430664, 0.3783113360404968),\n",
       " (3.67798113822937, 0.07265952974557877))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TS\n",
    "model_temp = ModelWithTemperature(model, initial_temperature=initial_temperature) \n",
    "model_temp.set_temperature(logits_valid, labels_valid)\n",
    "\n",
    "logits_valid_scaled_ts = model_temp.temperature_scale(logits_valid)      \n",
    "logits_test_scaled_ts = model_temp.temperature_scale(logits_test)     \n",
    "\n",
    "print(f\"# validation_size = {len(logits_valid)}\")\n",
    "get_nll_ece(logits_valid, logits_valid_scaled_ts, labels_valid)    \n",
    "print(f\"# test_size = {len(logits_test)}\")\n",
    "get_nll_ece(logits_test, logits_test_scaled_ts, labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal temperature:  2.180772542953491\n",
      "# validation_size = 200\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.50194 | 3.86166 |\n",
      "| ECE | 0.36361 | 0.08626 |\n",
      "+-----+---------+---------+\n",
      "# test_size = 9800\n",
      "+-----+---------+---------+\n",
      "|     | Before  |  After  |\n",
      "+-----+---------+---------+\n",
      "| NLL | 5.17957 | 3.67766 |\n",
      "| ECE | 0.37831 | 0.07607 |\n",
      "+-----+---------+---------+\n",
      "CPU times: user 53.7 s, sys: 91.9 ms, total: 53.8 s\n",
      "Wall time: 53.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5.179574966430664, 0.3783113360404968),\n",
       " (3.6776621341705322, 0.07607250660657883))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# ATS\n",
    "model_ATS = ATS(initial_temperature=initial_temperature)\n",
    "T_ATS = model_ATS.find_best_T(logits_valid, labels_valid, threshold=1e-5)\n",
    "print(\"Optimal temperature: \",T_ATS.item())\n",
    "logits_valid_scaled_ats = logits_valid/T_ATS.cuda()\n",
    "logits_test_scaled_ats = logits_test/T_ATS.cuda()\n",
    "\n",
    "print(f\"# validation_size = {len(logits_valid)}\")\n",
    "get_nll_ece(logits_valid, logits_valid_scaled_ats, labels_valid)\n",
    "print(f\"# test_size = {len(logits_test)}\")\n",
    "get_nll_ece(logits_test, logits_test_scaled_ats, labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
